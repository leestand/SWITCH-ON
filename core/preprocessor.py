"""
ì¿¼ë¦¬ ì „ì²˜ë¦¬ ëª¨ë“ˆ - ì¼ìƒì–´ë¥¼ ë²•ë¥ ì–´ë¡œ ë³€í™˜
"""
from functools import lru_cache
from langchain_openai import ChatOpenAI
from config.settings import LLM_MODEL


class LegalQueryPreprocessor:
    """ì¼ìƒì–´ë¥¼ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=0.1,  # ì¼ê´€ëœ ë³€í™˜ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
            max_tokens=200,
        )
        
        # ìºì‹œë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ (ì„¸ì…˜ ë™ì•ˆ ìœ ì§€)
        self._query_cache = {}
        
        # ê¸°ë³¸ ìš©ì–´ ë§¤í•‘ (ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë£°ë² ì´ìŠ¤)
        self.term_mapping = {
            # ë¶€ë™ì‚° ê´€ë ¨
            "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸",
            "ì„¸ì…ì": "ì„ì°¨ì¸", 
            "ì „ì„¸ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ",
            "ë³´ì¦ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ",
            "ì›”ì„¸": "ì°¨ì„",
            "ë°©ì„¸": "ì°¨ì„",
            "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì„œ",
            "ì§‘ ë‚˜ê°€ë¼": "ëª…ë„ì²­êµ¬",
            "ì«“ê²¨ë‚˜ë‹¤": "ëª…ë„",
            "ëˆ ì•ˆì¤˜": "ì±„ë¬´ë¶ˆì´í–‰",
            "ëˆ ëª»ë°›ì•„": "ë³´ì¦ê¸ˆë°˜í™˜ì²­êµ¬",
            "ì‚¬ê¸°": "ì‚¬ê¸°ì£„",
            "ì†ì•˜ë‹¤": "ê¸°ë§í–‰ìœ„",
            "ê¹¡í†µì „ì„¸": "ì „ì„¸ì‚¬ê¸°",
            "ì´ì¤‘ê³„ì•½": "ì¤‘ë³µì„ëŒ€",
            
            # ë²•ì  ì ˆì°¨ ê´€ë ¨
            "ê³ ì†Œ": "í˜•ì‚¬ê³ ë°œ",
            "ê³ ë°œ": "í˜•ì‚¬ê³ ë°œ", 
            "ì†Œì†¡": "ë¯¼ì‚¬ì†Œì†¡",
            "ì¬íŒ": "ì†Œì†¡",
            "ë³€í˜¸ì‚¬": "ë²•ë¬´ì‚¬",
            "ìƒë‹´": "ë²•ë¥ ìƒë‹´",
            "í•´ê²°": "ë¶„ìŸí•´ê²°",
            "ë³´ìƒ": "ì†í•´ë°°ìƒ",
            "ë°°ìƒ": "ì†í•´ë°°ìƒ",
            
            # ê¸°íƒ€
            "ê³„ì•½": "ë²•ë¥ í–‰ìœ„",
            "ì•½ì†": "ê³„ì•½",
            "ìœ„ë°˜": "ì±„ë¬´ë¶ˆì´í–‰",
            "ì–´ê¸°ë‹¤": "ìœ„ë°˜í•˜ë‹¤"
        }
    
    def _apply_rule_based_conversion(self, query: str) -> str:
        """ë£°ë² ì´ìŠ¤ ìš©ì–´ ë³€í™˜ (ë¹ ë¥¸ ì²˜ë¦¬)"""
        converted_query = query
        for common_term, legal_term in self.term_mapping.items():
            if common_term in converted_query:
                converted_query = converted_query.replace(common_term, legal_term)
        return converted_query
    
    def _is_already_legal_query(self, query: str) -> bool:
        """ì´ë¯¸ ë²•ë¥  ìš©ì–´ê°€ í¬í•¨ëœ ì¿¼ë¦¬ì¸ì§€ í™•ì¸"""
        legal_indicators = [
            "ì„ëŒ€ì¸", "ì„ì°¨ì¸", "ì„ëŒ€ì°¨", "ëª…ë„", "ì±„ë¬´ë¶ˆì´í–‰", 
            "ì†í•´ë°°ìƒ", "ë¯¼ì‚¬ì†Œì†¡", "í˜•ì‚¬ê³ ë°œ", "ë³´ì¦ê¸ˆë°˜í™˜",
            "ë²•ë¥ ", "íŒë¡€", "ë²•ë ¹", "ì†Œì†¡", "ê³„ì•½ì„œ"
        ]
        return any(term in query for term in legal_indicators)
    
    @lru_cache(maxsize=100)
    def _gpt_convert_to_legal_terms(self, user_query: str) -> str:
        """GPTë¥¼ ì‚¬ìš©í•œ ì •êµí•œ ë²•ë¥  ìš©ì–´ ë³€í™˜ (ìºì‹± ì ìš©)"""
        try:
            prompt = f"""ë‹¤ìŒ ì¼ìƒì–´ ì§ˆë¬¸ì„ ë²•ë¥  ê²€ìƒ‰ì— ì í•©í•œ ì „ë¬¸ ìš©ì–´ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
            
            ì›ë˜ ì§ˆë¬¸: {user_query}

            ë³€í™˜ ê·œì¹™:
            1. ì¼ìƒì–´ë¥¼ ì •í™•í•œ ë²•ë¥  ìš©ì–´ë¡œ ë°”ê¾¸ê¸°
            - ì§‘ì£¼ì¸ â†’ ì„ëŒ€ì¸
            - ì„¸ì…ì â†’ ì„ì°¨ì¸  
            - ì „ì„¸ê¸ˆ/ë³´ì¦ê¸ˆ â†’ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ
            - ì›”ì„¸ â†’ ì°¨ì„
            - ê³„ì•½ì„œ â†’ ì„ëŒ€ì°¨ê³„ì•½ì„œ
            - ì‚¬ê¸° â†’ ì „ì„¸ì‚¬ê¸° ë˜ëŠ” ì‚¬ê¸°ì£„
            - ì«“ê²¨ë‚˜ë‹¤ â†’ ëª…ë„ì²­êµ¬

            2. í•µì‹¬ ë²•ì  ìŸì ì„ ë¶€ê°ì‹œí‚¤ê¸°
            3. ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ëŠ” ê´€ë ¨ ë²•ë¥  í‚¤ì›Œë“œ ì¶”ê°€
            4. ì›ë˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ ë” ì •í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ í‘œí˜„

            ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:"""

            messages = [{"role": "user", "content": prompt}]
            response = self.llm.invoke(messages)
            
            # ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            converted = response.content.strip()
            if "ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:" in converted:
                converted = converted.split("ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:")[-1].strip()
            
            return converted
            
        except Exception as e:
            print(f"âš ï¸ GPT ë³€í™˜ ì‹¤íŒ¨, ë£°ë² ì´ìŠ¤ ë³€í™˜ ì‚¬ìš©: {e}")
            return self._apply_rule_based_conversion(user_query)
    
    def convert_query(self, user_query: str) -> tuple[str, str]:
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë²•ë¥  ê²€ìƒ‰ì— ì í•©í•˜ê²Œ ë³€í™˜
        
        Returns:
            tuple: (ë³€í™˜ëœ_ì¿¼ë¦¬, ë³€í™˜_ë°©ë²•)
        """
        try:
            # 1. ì´ë¯¸ ë²•ë¥  ìš©ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if self._is_already_legal_query(user_query):
                return user_query, "no_conversion"
            
            # 2. ìºì‹œ í™•ì¸
            if user_query in self._query_cache:
                return self._query_cache[user_query], "cached"
            
            # 3. ë¨¼ì € ë£°ë² ì´ìŠ¤ ë³€í™˜ ì‹œë„
            rule_converted = self._apply_rule_based_conversion(user_query)
            
            # 4. ë£°ë² ì´ìŠ¤ ë³€í™˜ìœ¼ë¡œ ì¶©ë¶„í•œ ê²½ìš° (ë§ì€ ë³€í™˜ì´ ì¼ì–´ë‚œ ê²½ìš°)
            if len(rule_converted) != len(user_query) or rule_converted != user_query:
                # ë£°ë² ì´ìŠ¤ ë³€í™˜ì´ íš¨ê³¼ê°€ ìˆì—ˆë‹¤ë©´ ê²°ê³¼ ìºì‹±
                self._query_cache[user_query] = rule_converted
                return rule_converted, "rule_based"
            
            # 5. ë³µì¡í•œ ê²½ìš° GPT ë³€í™˜ (ì‹œê°„ì´ ë” ê±¸ë¦¬ì§€ë§Œ ì •í™•í•¨)
            print("ğŸ”„ ì •êµí•œ ë²•ë¥  ìš©ì–´ ë³€í™˜ ì¤‘...")
            gpt_converted = self._gpt_convert_to_legal_terms(user_query)
            
            # ê²°ê³¼ ìºì‹±
            self._query_cache[user_query] = gpt_converted
            return gpt_converted, "gpt_converted"
            
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return user_query, "error"
